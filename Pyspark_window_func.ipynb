{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ae428e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|Department|Salary|\n",
      "+-------+----------+------+\n",
      "|  James|     Sales|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "| Robert|     Sales|  4100|\n",
      "|  Maria|   Finance|  3000|\n",
      "|  Raman|   Finance|  3000|\n",
      "|  Scott|   Finance|  3300|\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|  Kumar| Marketing|  2000|\n",
      "|    Jim|     Sales|  2400|\n",
      "|  Kumar|     Sales|  3000|\n",
      "+-------+----------+------+\n",
      "\n",
      "+-------+----------+------+----------+\n",
      "|   Name|Department|Salary|row_number|\n",
      "+-------+----------+------+----------+\n",
      "|  Maria|   Finance|  3000|         1|\n",
      "|  Raman|   Finance|  3000|         2|\n",
      "|  Scott|   Finance|  3300|         3|\n",
      "|    Jen|   Finance|  3900|         4|\n",
      "|  Kumar| Marketing|  2000|         1|\n",
      "|   Jeff| Marketing|  3000|         2|\n",
      "|    Jim|     Sales|  2400|         1|\n",
      "|  James|     Sales|  3000|         2|\n",
      "|  Kumar|     Sales|  3000|         3|\n",
      "| Robert|     Sales|  4100|         4|\n",
      "|Michael|     Sales|  4600|         5|\n",
      "+-------+----------+------+----------+\n",
      "\n",
      "+-------+----------+------+---+\n",
      "|   Name|Department|Salary|row|\n",
      "+-------+----------+------+---+\n",
      "|    Jen|   Finance|  3900|  1|\n",
      "|   Jeff| Marketing|  3000|  1|\n",
      "|Michael|     Sales|  4600|  1|\n",
      "+-------+----------+------+---+\n",
      "\n",
      "+-----+----------+------+---+\n",
      "| Name|Department|Salary|row|\n",
      "+-----+----------+------+---+\n",
      "|Maria|   Finance|  3000|  1|\n",
      "|Kumar| Marketing|  2000|  1|\n",
      "|  Jim|     Sales|  2400|  1|\n",
      "+-----+----------+------+---+\n",
      "\n",
      "+-------+----------+------+---+\n",
      "|   Name|Department|Salary|row|\n",
      "+-------+----------+------+---+\n",
      "|    Jen|   Finance|  3900|  1|\n",
      "|  Scott|   Finance|  3300|  2|\n",
      "|  Maria|   Finance|  3000|  3|\n",
      "|   Jeff| Marketing|  3000|  1|\n",
      "|  Kumar| Marketing|  2000|  2|\n",
      "|Michael|     Sales|  4600|  1|\n",
      "| Robert|     Sales|  4100|  2|\n",
      "|  James|     Sales|  3000|  3|\n",
      "+-------+----------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession,Row\n",
    "spark = SparkSession.builder.appName('Windows Function').getOrCreate()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "data = [(\"James\",\"Sales\",3000),(\"Michael\",\"Sales\",4600),\n",
    "      (\"Robert\",\"Sales\",4100),(\"Maria\",\"Finance\",3000),\n",
    "      (\"Raman\",\"Finance\",3000),(\"Scott\",\"Finance\",3300),\n",
    "      (\"Jen\",\"Finance\",3900),(\"Jeff\",\"Marketing\",3000),\n",
    "      (\"Kumar\",\"Marketing\",2000),(\"Jim\",\"Sales\",2400),\n",
    "       (\"Kumar\",\"Sales\",3000)]\n",
    "\n",
    "df = spark.createDataFrame(data,[\"Name\",\"Department\",\"Salary\"])\n",
    "df.show()\n",
    "\n",
    "# Add new column row_number\n",
    "wdf=Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\",row_number().over(wdf)).show()\n",
    "\n",
    "## Max salary in each department\n",
    "wdf = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row\",row_number().over(wdf)).filter(col(\"row\") ==1).show()\n",
    "\n",
    "\n",
    "## Min salary in each department\n",
    "wdf = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row\",row_number().over(wdf)).filter(col(\"row\") ==1).show()\n",
    "\n",
    "\n",
    "## Top 3 salary in each department\n",
    "wdf = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.withColumn(\"row\",row_number().over(wdf)).filter(col(\"row\") <=3).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6764f07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|   Name|Department|Salary|\n",
      "+-------+----------+------+\n",
      "|    Jen|   Finance|  3900|\n",
      "|   Jeff| Marketing|  3000|\n",
      "|Michael|     Sales|  4600|\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max salary using max function\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Define a window specification partitioned by department\n",
    "windowSpec = Window.partitionBy(\"Department\")\n",
    "\n",
    "# Add a column for the maximum salary within each department\n",
    "df_max_salary = df.withColumn(\"max_salary\", max(col(\"Salary\")).over(windowSpec))\n",
    "\n",
    "# Filter the DataFrame to retain only rows where the salary matches the maximum salary within its department\n",
    "result = df_max_salary.filter(col(\"Salary\") == col(\"max_salary\")).drop(\"max_salary\")\n",
    "\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5efc44cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3300.0|13200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3420.0|17100|2400|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get max, min, avg, sum of each group\n",
    "from pyspark.sql.functions import col, row_number,avg,sum,min,max\n",
    "w4 = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(wdf)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(w4)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(w4)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebc7b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   Name|Department|Salary|rank|\n",
      "+-------+----------+------+----+\n",
      "|    Jen|   Finance|  3900|   1|\n",
      "|  Scott|   Finance|  3300|   2|\n",
      "|  Maria|   Finance|  3000|   3|\n",
      "|  Raman|   Finance|  3000|   3|\n",
      "|   Jeff| Marketing|  3000|   1|\n",
      "|  Kumar| Marketing|  2000|   2|\n",
      "|Michael|     Sales|  4600|   1|\n",
      "| Robert|     Sales|  4100|   2|\n",
      "|  James|     Sales|  3000|   3|\n",
      "|  Kumar|     Sales|  3000|   3|\n",
      "|    Jim|     Sales|  2400|   5|\n",
      "+-------+----------+------+----+\n",
      "\n",
      "+-------+----------+------+----------+\n",
      "|   Name|Department|Salary|dense_rank|\n",
      "+-------+----------+------+----------+\n",
      "|    Jen|   Finance|  3900|         1|\n",
      "|  Scott|   Finance|  3300|         2|\n",
      "|  Maria|   Finance|  3000|         3|\n",
      "|  Raman|   Finance|  3000|         3|\n",
      "|   Jeff| Marketing|  3000|         1|\n",
      "|  Kumar| Marketing|  2000|         2|\n",
      "|Michael|     Sales|  4600|         1|\n",
      "| Robert|     Sales|  4100|         2|\n",
      "|  James|     Sales|  3000|         3|\n",
      "|  Kumar|     Sales|  3000|         3|\n",
      "|    Jim|     Sales|  2400|         4|\n",
      "+-------+----------+------+----------+\n",
      "\n",
      "+-------+----------+------+------------------+\n",
      "|   Name|Department|Salary|      percent_rank|\n",
      "+-------+----------+------+------------------+\n",
      "|    Jen|   Finance|  3900|               0.0|\n",
      "|  Scott|   Finance|  3300|0.3333333333333333|\n",
      "|  Maria|   Finance|  3000|0.6666666666666666|\n",
      "|  Raman|   Finance|  3000|0.6666666666666666|\n",
      "|   Jeff| Marketing|  3000|               0.0|\n",
      "|  Kumar| Marketing|  2000|               1.0|\n",
      "|Michael|     Sales|  4600|               0.0|\n",
      "| Robert|     Sales|  4100|              0.25|\n",
      "|  James|     Sales|  3000|               0.5|\n",
      "|  Kumar|     Sales|  3000|               0.5|\n",
      "|    Jim|     Sales|  2400|               1.0|\n",
      "+-------+----------+------+------------------+\n",
      "\n",
      "+-------+----------+------+-----+\n",
      "|   Name|Department|Salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|    Jen|   Finance|  3900|    1|\n",
      "|  Scott|   Finance|  3300|    1|\n",
      "|  Maria|   Finance|  3000|    2|\n",
      "|  Raman|   Finance|  3000|    2|\n",
      "|   Jeff| Marketing|  3000|    1|\n",
      "|  Kumar| Marketing|  2000|    2|\n",
      "|Michael|     Sales|  4600|    1|\n",
      "| Robert|     Sales|  4100|    1|\n",
      "|  James|     Sales|  3000|    1|\n",
      "|  Kumar|     Sales|  3000|    2|\n",
      "|    Jim|     Sales|  2400|    2|\n",
      "+-------+----------+------+-----+\n",
      "\n",
      "+-------+----------+------+---------+\n",
      "|   Name|Department|Salary|cume_dist|\n",
      "+-------+----------+------+---------+\n",
      "|    Jen|   Finance|  3900|     0.25|\n",
      "|  Scott|   Finance|  3300|      0.5|\n",
      "|  Maria|   Finance|  3000|      1.0|\n",
      "|  Raman|   Finance|  3000|      1.0|\n",
      "|   Jeff| Marketing|  3000|      0.5|\n",
      "|  Kumar| Marketing|  2000|      1.0|\n",
      "|Michael|     Sales|  4600|      0.2|\n",
      "| Robert|     Sales|  4100|      0.4|\n",
      "|  James|     Sales|  3000|      0.8|\n",
      "|  Kumar|     Sales|  3000|      0.8|\n",
      "|    Jim|     Sales|  2400|      1.0|\n",
      "+-------+----------+------+---------+\n",
      "\n",
      "+-------+----------+------+----+\n",
      "|   Name|Department|Salary| lag|\n",
      "+-------+----------+------+----+\n",
      "|    Jen|   Finance|  3900|null|\n",
      "|  Scott|   Finance|  3300|3900|\n",
      "|  Maria|   Finance|  3000|3300|\n",
      "|  Raman|   Finance|  3000|3000|\n",
      "|   Jeff| Marketing|  3000|null|\n",
      "|  Kumar| Marketing|  2000|3000|\n",
      "|Michael|     Sales|  4600|null|\n",
      "| Robert|     Sales|  4100|4600|\n",
      "|  James|     Sales|  3000|4100|\n",
      "|  Kumar|     Sales|  3000|3000|\n",
      "|    Jim|     Sales|  2400|3000|\n",
      "+-------+----------+------+----+\n",
      "\n",
      "+-------+----------+------+----+\n",
      "|   Name|Department|Salary|lead|\n",
      "+-------+----------+------+----+\n",
      "|    Jen|   Finance|  3900|3300|\n",
      "|  Scott|   Finance|  3300|3000|\n",
      "|  Maria|   Finance|  3000|3000|\n",
      "|  Raman|   Finance|  3000|null|\n",
      "|   Jeff| Marketing|  3000|2000|\n",
      "|  Kumar| Marketing|  2000|null|\n",
      "|Michael|     Sales|  4600|4100|\n",
      "| Robert|     Sales|  4100|3000|\n",
      "|  James|     Sales|  3000|3000|\n",
      "|  Kumar|     Sales|  3000|2400|\n",
      "|    Jim|     Sales|  2400|null|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The RANK() function assigns a unique rank to each distinct row within a result set.\n",
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(wdf)).show()\n",
    "\n",
    "# DENSE_RANK() ensures that the ranks are continuous without any gaps, regardless of ties. \n",
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(wdf)).show()\n",
    "\n",
    "# Ranking in the form of percentage: rank=1, percent_rank=0.0: rank=2 out of 4 ranks, percent_rank=0.25\n",
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(wdf)).show()\n",
    "    \n",
    "# The NTILE() function is a window function that distributes rows of an ordered partition \n",
    "# into a specified number of approximately equal groups, or buckets.\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(wdf)).show()\n",
    "\n",
    "# CUME_DIST returns the cumulative distribution of a value in a group of values.\n",
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(wdf)).show()\n",
    "\n",
    "# creates a new column that accesses a previous row from another column.\n",
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",1).over(wdf)).show()\n",
    "\n",
    "\n",
    "# creates a new column that accesses a next row from current column.\n",
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",1).over(wdf)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b1836d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   Name|Department|Salary|lead|\n",
      "+-------+----------+------+----+\n",
      "|    Jen|   Finance|  3900|3900|\n",
      "|  Scott|   Finance|  3300|3900|\n",
      "|  Maria|   Finance|  3000|3900|\n",
      "|  Raman|   Finance|  3000|3900|\n",
      "|   Jeff| Marketing|  3000|3000|\n",
      "|  Kumar| Marketing|  2000|3000|\n",
      "|Michael|     Sales|  4600|4600|\n",
      "| Robert|     Sales|  4100|4600|\n",
      "|  James|     Sales|  3000|4600|\n",
      "|  Kumar|     Sales|  3000|4600|\n",
      "|    Jim|     Sales|  2400|4600|\n",
      "+-------+----------+------+----+\n",
      "\n",
      "+-------+----------+------+----+\n",
      "|   Name|Department|Salary|lead|\n",
      "+-------+----------+------+----+\n",
      "|    Jen|   Finance|  3900|3900|\n",
      "|  Scott|   Finance|  3300|3300|\n",
      "|  Maria|   Finance|  3000|3000|\n",
      "|  Raman|   Finance|  3000|3000|\n",
      "|   Jeff| Marketing|  3000|3000|\n",
      "|  Kumar| Marketing|  2000|2000|\n",
      "|Michael|     Sales|  4600|4600|\n",
      "| Robert|     Sales|  4100|4100|\n",
      "|  James|     Sales|  3000|3000|\n",
      "|  Kumar|     Sales|  3000|3000|\n",
      "|    Jim|     Sales|  2400|2400|\n",
      "+-------+----------+------+----+\n",
      "\n",
      "+-------+----------+------+------------------+\n",
      "|   Name|Department|Salary|              lead|\n",
      "+-------+----------+------+------------------+\n",
      "|    Jen|   Finance|  3900|              null|\n",
      "|  Scott|   Finance|  3300|424.26406871192853|\n",
      "|  Maria|   Finance|  3000|424.26406871192853|\n",
      "|  Raman|   Finance|  3000|424.26406871192853|\n",
      "|   Jeff| Marketing|  3000|              null|\n",
      "|  Kumar| Marketing|  2000| 707.1067811865476|\n",
      "|Michael|     Sales|  4600|              null|\n",
      "| Robert|     Sales|  4100| 353.5533905932738|\n",
      "|  James|     Sales|  3000|  805.708797684788|\n",
      "|  Kumar|     Sales|  3000|  805.708797684788|\n",
      "|    Jim|     Sales|  2400| 901.1104260855047|\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First provides the max value in the given group displayed as new column\n",
    "from pyspark.sql.functions import first    \n",
    "df.withColumn(\"first\",first(\"salary\").over(wdf)).show()\n",
    "\n",
    "\n",
    "# Last provides the min value in the given group displayed as new column\n",
    "from pyspark.sql.functions import last    \n",
    "df.withColumn(\"last\",last(\"salary\").over(wdf)).show()\n",
    "\n",
    "\n",
    "# standard deviation\n",
    "from pyspark.sql.functions import stddev    \n",
    "df.withColumn(\"stddev\",stddev(\"salary\").over(wdf)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d45b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
