{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c56ae3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Pyspark Join Intro:Joining DataFrames is a common and powerful operation in PySpark, \n",
    "# allowing you to combine datasets for richer analysis.\n",
    "\n",
    "# Types of Joins: Inner,Left outer, Self, Right Outer, full outer, left-semi, Left-anti, Cross\n",
    "\n",
    "# Pyspark Join Performance : Performance is a critical consideration when performing joins in PySpark, \n",
    "# especially when dealing with large datasets. \n",
    "# Here are key strategies and considerations to optimize the performance of joins in PySpark:\n",
    "# 1. Data Partitioning : a) Shuffling: Avoid shuffling by partitioning the data.\n",
    "#                        b) Broadcast Join: When one of the dataframe is small, use broadcast(df_small)\n",
    "# 2. Use Appropriate Join Types : Avoid cross joins, use left-semi/left-anti whenever fits in.\n",
    "# 3. Pushdown Filters : Apply filters before join to reduce data that needs to be transformed.\n",
    "# 4. Avoid Skewness: Use salting technique\n",
    "# 5. Repartitioning: any column is used in frequent join conditions, repartition(\"key\") to redistribute the data.\n",
    "# 6. Cache/Persist: when the joined df is used multiple times, cache or persist the data to avoid recomputing.\n",
    "#                  df1.join(df2,\"key\").cache()\n",
    "# 7. Optimize Join Algorithms: a) Sort-Merge Join: works well when both DataFrames are large and sorted on the join keys.\n",
    "#                              b) Broadcast Hash Join: used when one of the DataFrames is small enough to be broadcasted\n",
    "#                                 to all nodes. \n",
    "# 8. Tuning Spark Configurations: a) Partition Size: Adjust spark.sql.shuffle.partitions based on your data size \n",
    "#                                    and cluster resources. The default value might not be optimal for large datasets.\n",
    "#                                 b) Executor Memory: Ensure that your executors have enough memory to handle the DataFrames\n",
    "#                        being joined. Too little memory can lead to excessive garbage collection or out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b036af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n",
      "+------+--------+---------------+-----------------+\n",
      "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
      "+------+--------+---------------+-----------------+\n",
      "|2     |Rose    |1              |Smith            |\n",
      "|3     |Williams|1              |Smith            |\n",
      "|4     |Jones   |2              |Rose             |\n",
      "|5     |Brown   |2              |Rose             |\n",
      "|6     |Brown   |2              |Rose             |\n",
      "+------+--------+---------------+-----------------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Joins').master('local[*]').getOrCreate()\n",
    "\n",
    "#creating employee dataframe\n",
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),\n",
    " (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000),\n",
    " (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),\n",
    " (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000),\n",
    " (5,\"Brown\",2,\"2010\",\"40\",\"\",-1),\n",
    " (6,\"Brown\",2,\"2010\",\"50\",\"\",-1)\n",
    " ]\n",
    "\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\",\"emp_dept_id\",\"gender\",\"salary\"]\n",
    "empDF = spark.createDataFrame(data=emp,schema=empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "#creating Department dataframe\n",
    "dept = [(\"Finance\",10),\n",
    " (\"Marketing\",20),\n",
    " (\"Sales\",30),\n",
    " (\"IT\",40)\n",
    " ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data = dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate = False)\n",
    "#Inner Join\n",
    "print(\"Inner join result\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"inner\").show(truncate=False)\n",
    "#Left Outer Join\n",
    "print(\"Left Outer join result\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"left\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"leftouter\").show(truncate=False)\n",
    "#Right Outer Join\n",
    "print(\"Right Outer join result\")\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"right\").show(truncate=False)\n",
    "empDF.join(deptDF,empDF.emp_dept_id==deptDF.dept_id,\"rightouter\").show(truncate=False)\n",
    "# Full Outer Join\n",
    "print(\"Full Outer join result\")\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"outer\").show(truncate = False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"full\").show(truncate = False)\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"fullouter\").show(truncate = False)\n",
    "# Left Semi Join\n",
    "print(\"Left Semi join result\")\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftsemi\").show(truncate = False)\n",
    "# Left Anti Join\n",
    "print(\"Left Anti join result\")\n",
    "empDF.join(deptDF, empDF.emp_dept_id == deptDF.dept_id, \"leftanti\").show(truncate = False)\n",
    "# Self Join\n",
    "print(\"Self join result\")\n",
    "from pyspark.sql.functions import col\n",
    "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"),col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"), \"inner\") \\\n",
    ".select(col(\"emp1.emp_id\"), col(\"emp1.name\"),col(\"emp2.emp_id\").alias(\"superior_emp_id\"),col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
    ".show(truncate = False)\n",
    "# Using spark.sql\n",
    "print(\"Spark SQL result\")\n",
    "empDF.createOrReplaceTempView(\"EMP\")\n",
    "deptDF.createOrReplaceTempView(\"DEPT\")\n",
    "joinDF = spark.sql(\"SELECT * FROM EMP e, DEPT d WHERE e.emp_dept_id == d.dept_id\").show(truncate = False) \n",
    "joinDF2 = spark.sql(\"SELECT * FROM EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\").show(truncate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec116f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|\n",
      "|     6|   Brown|              2|       2010|         50|      |    -1|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "8\n",
      "2\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "empDF.join(broadcast(deptDF),empDF.emp_dept_id == deptDF.dept_id,\"inner\").show(truncate = False)\n",
    "\n",
    "# cache/persist:MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "cachDf=empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id,\"inner\").cache()\n",
    "\n",
    "empDF.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "empDF.show()\n",
    "\n",
    "empDF.unpersist()\n",
    "\n",
    "# repartition and coalesce\n",
    "\n",
    "print(empDF.rdd.getNumPartitions())  #8\n",
    "coDF=empDF.coalesce(2)\n",
    "\n",
    "print(coDF.rdd.getNumPartitions()) #2\n",
    "\n",
    "print(deptDF.rdd.getNumPartitions()) #8\n",
    "repDF=deptDF.repartition(10)\n",
    "print(repDF.rdd.getNumPartitions()) #10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5089329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# Assume df1 is skewed on the \"key\" column\n",
    "# Adding a salt column to df1 and df2\n",
    "\n",
    "# Step 1: Create a random salt column in df1\n",
    "empSalt = empDF.withColumn(\"salt\", (expr(\"floor(rand() * 5)\")))\n",
    "\n",
    "# Step 2: Create the same salt values in df2\n",
    "# Here, we need to replicate the rows in df2 across the possible salt values\n",
    "deptSalt = deptDF.crossJoin(spark.range(0, 5).toDF(\"salt\"))\n",
    "\n",
    "# Step 3: Perform the join on both the key and the salt columns\n",
    "result = empSalt.join(deptSalt, (empSalt.emp_dept_id == deptSalt.dept_id) & (empSalt.salt == deptSalt.salt))\n",
    "\n",
    "# Step 4: Drop the salt column after the join\n",
    "result.drop(\"salt\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da11003",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
